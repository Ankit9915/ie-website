{% extends "base.html" %}
{% block virtual_expo %}active{% endblock %}
{% load static %}
{% block main %}


<section class="probootstrap-section"
    style="background-image: url('{% static 'img/virtual-expo/ImageCaptioning.jpeg' %}'); width: 100%;background-size: cover;">
    <div class="container">
        <div class="row">
            <div class="col-md-12 text-left section-heading probootstrap-animate ">
                <!-- <h1>
                    <font color="black">Image Captioning</font>
                </h1> -->
            </div>
        </div>
    </div>
</section>

<section class="probootstrap-section probootstrap-section-sm">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h3>PROBLEM STATEMENT: </h3>
                <p>Automatically describing the content of images using natural languages is a fundamental and
                    challenging task. It has great potential impact. For example, it could help visually impaired
                    people better understand the content of images on the web. Also, it could provide more
                    accurate and compact information of images/videos in scenarios such as image sharing in
                    social networks or video surveillance systems. This project accomplishes this task using deep
                    neural networks. By learning knowledge from image and caption pairs, the method can generate
                    image captions that are usually semantically descriptive and grammatically correct.<br><br>

                    This application bridges vision and natural language. If we can do well in this task, we can then
                    utilize natural language processing technologies to understand the world in images. In addition,

                    we introduced an attention mechanism, which is able to recognize what a word refers to in the
                    image, and thus summarize the relationship between objects in the image. This will be a
                    powerful tool to utilize the massive unformatted image data, which dominates the whole data in
                    the world.
                </p>


                <h3>PROPOSED SOLUTION:</h3>
                <p>In this project, we develop a framework leveraging the capabilities of artificial neural networks to
                    "caption an image based on its significant features' '. Recurrent Neural Networks (RNN) are
                    increasingly being used as encoding-decoding frameworks for machine translation. Our objective is
                    to replace the encoder part of the RNN with a Convolutional Neural Network (CNN). Thus,
                    transforming the images into relevant input data to feed into the decoder of the RNN. The image will
                    be converted into a multi-feature dataset, characterizing its distinctive features. The analysis
                    will be
                    carried out on the popular Flickr 8K dataset.
                </p>


                <h3>TECHNOLOGY USED</h3>
                <p>Convolutional Neural Networks, InceptionV3 Model, Deep Learning, Keras,
                    Natural Language Processing, Google Colab, Python, Tensorflow, Flickr 8K dataset(training and
                    testing)
                </p>




                <h3>METHODOLOGY</h3>
                <p>1. Collect the data set, clean the Caption Data using Python.<br>
                    2. Pre-process the images using InceptionV3 Model and convert them to a vector.<br>
                    3. Pre-process the captions by tokenizing each unique word in the testing set.Hence
                    creating a vocabulary of words.<br>
                    4. Use a pretrained model with weight to detect objects in the images.<br>
                    5. Train the Model using CNN with above pre -processed images and captions.<br>
                    6. Grammatically correct captions are generated word by word using techniques of Natural
                    Language processing.<br>
                    7. The above weights are saved in a pickle file to predict captions in the future.<br>
                </p>

                <h3>RESULTS</h3>
                <p>Few of our results can be seen in the below.
                    <br><br>
                    <img src="{% static 'img/virtual-expo/ImageCaptioning01.png' %}" class="img-rounded img-fluid"
                        style="width: 100%;">
                    <img src="{% static 'img/virtual-expo/ImageCaptioning02.png' %}" class="img-rounded img-fluid"
                        style="width: 100%;">
                    <img src="{% static 'img/virtual-expo/ImageCaptioning03.png' %}" class="img-rounded img-fluid"
                        style="width: 100%;">
                </p>

                <h3>DEMO</h3>
                <p>
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe width="789" height="386" src="https://www.youtube.com/embed/Cep8g9kRn4w" frameborder="0"
                        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                </div>
                </p>

                <h3>FUTURE WORK</h3>
                <p>1. Using a better and larger data set<br>
                    2. Doing hyper parameter tuning<br>
                    3. Trying different model architectures and choosing the best if any<br>
                    4. Using a greater loss function<br>
                </p>


                <h3>KEY LEARNINGS</h3>
                <p>Convolutional Neural Networks, InceptionV3 model, Natural Language Processing
                </p>


                <h3>CONCLUSION</h3>
                <p>We can see that the generated sentences expressed the pictures quite well. The main parts of
                    the images can be recognized and shown in the sentence, and also of the minor parts are also
                    encoded. Also, there are some mistakes in some images(fig 3), where the dog is predicted as
                    running . But we human beings are also easy to make such mistakes since there do exist similar
                    objects in the image. The generated sentences also do well in following grammar.
                </p>




                <h3>REFERENCES</h3>
                <p>
                    Following are the papers that we used as reference for our project:
                </p>
                <p>1. <a
                        href="https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8">https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8</a>
                </p>
                <p></p>

                <h3>TEAM</h3>

                <p>● Akshit Patel(Mentor) -akshitpatel01@gmail.com<br>
                    ● Niwedita(Mentor) -niwedita.dakshana2017@gmail.com<br>
                    ● Kshitij Raj -kshiteej.raj@gmail.com<br>
                    ● Shreesha Bharadwaj -bharadwajshreesha@gmail.com<br>
                </p>
                <p></p>

                <!-- <h3>PICTURES</h3>
                <section class="container">
                    <div class="row mb-5">

                        <div class="card-deck  col-md-4 col-sm-6 col-sm-12">
                            <div class="card" style="margin-bottom: 15px;">
                                <div style="text-align:center">

                                    <img src="" alt="Card image cap" class="img-fluid" style="height:270px; width:100%">
                                    <div class="card-body">
                                        <h6 class="card-title">card title</h4>
                                    </div>
                                </div>
                            </div>
                        </div>
 -->

            </div>
</section>
</div>
</div>
</div>
</section>











{% comment %} <section class="probootstrap-section probootstrap-section-sm">
    <div class="container">
        <div class="row">
            <div class="col-md-12">

            </div>
        </div>
    </div>
</section> {% endcomment %}

<script type="text/javascript">
    $(".team").addClass("active");
    var tabcontent = document.getElementsByClassName('teams');
    for (i = 1; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }

    function navigate(signame) {
        var tabcontent = document.getElementsByClassName('teams');
        for (i = 0; i < tabcontent.length; i++) {
            tabcontent[i].style.display = "none";
        }
        tablinks = document.getElementsByClassName("tablinks");
        for (i = 0; i < tablinks.length; i++) {
            tablinks[i].className = tablinks[i].className.replace(" active", "");
        }
        document.getElementById(signame).style.display = "block";
        $("." + signame).addClass("active");
    }
</script>
{% endblock %}